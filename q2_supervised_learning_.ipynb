{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:34:17.725534Z",
     "start_time": "2025-03-30T08:34:13.546725Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Classic Machine Learning Methods (5 Pts)\n",
    "#### Q2.1 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:34:20.505469Z",
     "start_time": "2025-03-30T08:34:20.356043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieving the X matrix\n",
    "df = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "df = df.fillna(0)\n",
    "X = df.groupby(\"RecordID\").last(numeric_only=True).reset_index()\n",
    "X = X.drop(columns=[\"RecordID\"])\n",
    "X = X[sorted(X.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:34:21.713479Z",
     "start_time": "2025-03-30T08:34:21.706037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieving the label vector\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-a.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "print(y.sum())\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:34:29.032583Z",
     "start_time": "2025-03-30T08:34:23.287398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model1 = LogisticRegression(max_iter=500, class_weight='balanced')\n",
    "model1.fit(X,y)\n",
    "\n",
    "# Random Forest\n",
    "model2 = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    min_samples_split=5,    \n",
    "    min_samples_leaf=2,         \n",
    "    class_weight='balanced',\n",
    "    random_state=42,        \n",
    ")\n",
    "model2.fit(X,y)\n",
    "\n",
    "# KNN\n",
    "model3 = KNeighborsClassifier(\n",
    "    n_neighbors=150,\n",
    "    weights='distance',\n",
    "    metric='manhattan',\n",
    ")\n",
    "model3.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:34:40.654635Z",
     "start_time": "2025-03-30T08:34:39.019668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test set C performance\n",
    "\n",
    "# Loading test set C\n",
    "df = pd.read_parquet('final-data/final-set-c.parquet')\n",
    "df = df.fillna(0)\n",
    "df = df.drop(columns=[\"ICUType\"])\n",
    "X_test = df.groupby(\"RecordID\").last(numeric_only=True).reset_index()\n",
    "X_test = X_test.drop(columns=[\"RecordID\"])\n",
    "X_test= X_test[sorted(X_test.columns)]\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y_test = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "y_pred1 = model1.predict_proba(X_test)[:,1]\n",
    "y_pred2 = model2.predict_proba(X_test)[:,1]\n",
    "y_pred3 = model3.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Logistic Regression\n",
    "print(\"Logistic Regression results\")\n",
    "auroc = roc_auc_score(y_test, y_pred1)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred1)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Random Forests\n",
    "print(\"Random Forests results\")\n",
    "auroc = roc_auc_score(y_test, y_pred2)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred2)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for KNN\n",
    "print(\"KNN results\")\n",
    "auroc = roc_auc_score(y_test, y_pred3)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred3)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.1 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:35:28.969221Z",
     "start_time": "2025-03-30T08:34:45.594091Z"
    }
   },
   "outputs": [],
   "source": [
    "extraction_settings = MinimalFCParameters()\n",
    "\n",
    "# Extracting features of concatenated training and test dataset (need to do this in one go so the feature extraction is consistent)\n",
    "df_train = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "df_test = pd.read_parquet('final-data/final-set-c.parquet').drop(columns=[\"ICUType\"])\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)\n",
    "\n",
    "X_train = extract_features(df_train, column_id='RecordID', column_sort='Time', default_fc_parameters=extraction_settings, impute_function=impute)\n",
    "X_test = extract_features(df_test, column_id='RecordID', column_sort='Time', default_fc_parameters=extraction_settings, impute_function=impute)\n",
    "\n",
    "X_train= X_train[sorted(X_train.columns)]\n",
    "X_test= X_test[sorted(X_test.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:35:48.915381Z",
     "start_time": "2025-03-30T08:35:48.905208Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet('processed-data/processed-outcomes-a.parquet')[\"In-hospital_death\"].to_numpy().flatten()\n",
    "y_test = pd.read_parquet('processed-data/processed-outcomes-c.parquet')[\"In-hospital_death\"].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:36:12.135292Z",
     "start_time": "2025-03-30T08:35:51.191691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "# Logistic Regression\n",
    "model1 = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
    "model1.fit(X_train,y_train)\n",
    "\n",
    "# Random Forest\n",
    "model2 = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    min_samples_split=5,    \n",
    "    min_samples_leaf=2,         \n",
    "    class_weight='balanced',\n",
    "    random_state=42,        \n",
    ")\n",
    "model2.fit(X_train,y_train)\n",
    "\n",
    "# KNN\n",
    "model3 = KNeighborsClassifier(\n",
    "    n_neighbors=200,\n",
    "    weights='distance',\n",
    "    metric='manhattan',\n",
    ")\n",
    "model3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:36:17.507193Z",
     "start_time": "2025-03-30T08:36:14.305749Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict_proba(X_test)[:,1]\n",
    "y_pred2 = model2.predict_proba(X_test)[:,1]\n",
    "y_pred3 = model3.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Logistic Regression\n",
    "print(\"Logistic Regression results\")\n",
    "auroc = roc_auc_score(y_test, y_pred1)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred1)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Random Forests\n",
    "print(\"Random Forests results\")\n",
    "auroc = roc_auc_score(y_test, y_pred2)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred2)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for KNN\n",
    "print(\"KNN results\")\n",
    "auroc = roc_auc_score(y_test, y_pred3)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred3)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Recurrent Neural Networks (4 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:36:20.515774Z",
     "start_time": "2025-03-30T08:36:20.512998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run in case CUDA_LAUNCH_BLOCKING error occurs\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T08:36:21.432831Z",
     "start_time": "2025-03-30T08:36:21.429003Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:04:21.111152Z",
     "start_time": "2025-03-30T09:04:21.105462Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.7)\n",
    "        self.bn = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Extracting output of last timestep\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "\n",
    "        out = self.fc(lstm_out)  \n",
    "        return out\n",
    "    \n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.7, bidirectional=True)\n",
    "\n",
    "        # Adjusted because of bidirectional LSTM\n",
    "        self.bn = nn.LayerNorm(hidden_size * 2)  \n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Extracting output of last timestep\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:04:23.742736Z",
     "start_time": "2025-03-30T09:04:23.737898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0003\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "##### NORMAL LSTM\n",
    "# Test AUROC: 0.8218, Test AUPRC: 0.4636 (only set a)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "num_epochs = 10 \"\"\"\n",
    "\n",
    "# Test AUROC: 0.7828, Test AUPRC: 0.4165 (set a and set b)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "num_epochs = 9 \"\"\"\n",
    "\n",
    "##### BIDIRECTIONAL LSTM\n",
    "# Test AUROC: 0.8227, Test AUPRC: 0.4557 (Only set a)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0003\n",
    "batch_size = 64\n",
    "num_epochs = 3 \"\"\"\n",
    "\n",
    "# Test AUROC: 0.7845, Test AUPRC: 0.4120 (set a and set b)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 4 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:04:27.588309Z",
     "start_time": "2025-03-30T09:04:26.023961Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: In the scaled-data set the time column is not scaled. However, due to better performance, we still scale it for this application. RecordID can be dropped, since it doesn't convey any further information and each patient is assigned one of the 4000 dimensions.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Training Data\n",
    "df = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "X = df.fillna(0)\n",
    "X = X.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X = X.sort_values(by=\"RecordID\", ascending=True)\n",
    "X[\"Time\"] = scaler.fit_transform(X[[\"Time\"]])\n",
    "X = X.drop(columns=[\"RecordID\"])\n",
    "X = X[sorted(X.columns)]\n",
    "X = X.to_numpy()\n",
    "X = X.reshape(4000,49,41)\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-a.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "train_dataset = PatientDataset(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Validation data\n",
    "df = pd.read_parquet('final-data/final-set-b.parquet')\n",
    "X = df.fillna(0)\n",
    "X = X.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X = X.sort_values(by=\"RecordID\", ascending=True)\n",
    "X[\"Time\"] = scaler.fit_transform(X[[\"Time\"]])\n",
    "X = X.drop(columns=[\"RecordID\"]).drop(columns=[\"ICUType\"])\n",
    "X = X[sorted(X.columns)]\n",
    "X = X.to_numpy()\n",
    "X = X.reshape(4000,49,41)\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "val_dataset = PatientDataset(X, y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Test data\n",
    "df = pd.read_parquet('final-data/final-set-c.parquet')\n",
    "X = df.fillna(0)\n",
    "X = X.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X = X.sort_values(by=\"RecordID\", ascending=True)\n",
    "X[\"Time\"] = scaler.fit_transform(X[[\"Time\"]])\n",
    "X = X.drop(columns=[\"RecordID\"]).drop(columns=[\"ICUType\"])\n",
    "X = X[sorted(X.columns)]\n",
    "X = X.to_numpy()\n",
    "X = X.reshape(4000,49,41)\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "test_dataset = PatientDataset(X, y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Full training (use after having found best model with lowest valildation error)\n",
    "df = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "X1 = df.fillna(0)\n",
    "X1 = X1.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X1 = X1.sort_values(by=\"RecordID\", ascending=True)\n",
    "X1[\"Time\"] = scaler.fit_transform(X1[[\"Time\"]])\n",
    "X1 = X1[sorted(X1.columns)]\n",
    "df = pd.read_parquet('final-data/final-set-b.parquet')\n",
    "X2 = df.fillna(0)\n",
    "X2 = X2.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X2 = X2.sort_values(by=\"RecordID\", ascending=True)\n",
    "X2[\"Time\"] = scaler.fit_transform(X2[[\"Time\"]])\n",
    "X2 = X2.drop(columns=[\"ICUType\"])\n",
    "X2 = X2[sorted(X2.columns)]\n",
    "\n",
    "X_full = pd.concat([X1, X2], axis=0).drop(columns=\"RecordID\")\n",
    "\n",
    "X_full = X_full.to_numpy()\n",
    "X_full = X_full.reshape(8000,49,41)\n",
    "\n",
    "y1 = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y1 = y1[\"In-hospital_death\"].to_numpy().flatten()\n",
    "y2 = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y2 = y2[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "y_full = np.concatenate((y1,y2), axis=0)\n",
    "\n",
    "full_dataset = PatientDataset(X_full, y_full)\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Uncomment/comment line depending on if you want to use normal LSTM or bidirectional LSTM\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "#model = BidirectionalLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and validation loss\n",
    "\n",
    "# NOTE: This is for validation error analysis\n",
    "\n",
    "total_val_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    total_val_loss += val_loss\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "total_val_loss /= num_epochs\n",
    "print(f\"Avg val loss: {total_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and AUROC/AUPRC\n",
    "\n",
    "# NOTE: Change train_loader with full_loader if we want to train the model on set a and set b (after validation error analysis)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # AUROC & AUPRC calculation\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3a Transformers (3 Pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:04:32.021182Z",
     "start_time": "2025-03-30T09:04:32.016377Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, d_model=128, num_heads=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, # Dimensionality of embeddings\n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # FC layer for classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:04:33.731319Z",
     "start_time": "2025-03-30T09:04:33.727351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = 41\n",
    "num_classes = 1 \n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 4 \n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 10\n",
    "\n",
    "# Test AUROC: 0.8338, Test AUPRC: 0.4719 (Only set a)\n",
    "\"\"\" input_size = 41\n",
    "num_classes = 1 \n",
    "d_model = 128\n",
    "num_heads = 16\n",
    "num_layers = 8\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 6 \"\"\"\n",
    "\n",
    "# Test AUROC: 0.8250, Test AUPRC: 0.4619 (set a and set b)\n",
    "\"\"\" input_size = 41\n",
    "num_classes = 1 \n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 4 \n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 3 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:04:35.601735Z",
     "start_time": "2025-03-30T09:04:35.583882Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(input_size, num_classes, d_model, num_heads, num_layers, dim_feedforward, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([7.0]).to(device))  # Handle class imbalance\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:10:32.766216Z",
     "start_time": "2025-03-30T09:04:36.733588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and AUROC/AUPRC\n",
    "\n",
    "# NOTE: Change train_loader with full_loader if we want to train the model on set a and set b (after validation error analysis)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(full_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(full_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Compute AUROC & AUPRC\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3b Tokenizing Time-Series Data and Transformers (4 Pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time conversion from \"XX:YY\" to an interval between [0,1]\n",
    "def scale_time(time_str):\n",
    "    # Split the time string into hours and minutes\n",
    "    hours, minutes = map(int, time_str.split(\":\"))\n",
    "    \n",
    "    # Convert the time to total minutes\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    \n",
    "    # Maximum time is 48 hours (2880 minutes)\n",
    "    max_time = 48 * 60\n",
    "    \n",
    "    # Scale time to the range [0, 1]\n",
    "    scaled_time = total_minutes / max_time\n",
    "    return scaled_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding of the different categories\n",
    "categories = [['ALP', 'ALT', 'AST', 'Age', 'Albumin', 'BUN', 'Bilirubin', 'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2',\n",
    "                'GCS', 'Gender', 'Glucose', 'HCO3', 'HCT', 'HR', 'Height', 'K', 'Lactate', 'MAP', 'MechVent', 'Mg', 'NIDiasABP',\n",
    "                'NIMAP', 'NISysABP', 'Na', 'PaCO2', 'PaO2', 'Platelets', 'RespRate', 'SaO2', 'SysABP', 'Temp', 'TroponinI',\n",
    "                'TroponinT', 'Urine', 'WBC', 'Weight', 'pH']]\n",
    "encoder = OneHotEncoder(categories=categories, sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(df):\n",
    "    # Ensure the \"Value\" column is numeric\n",
    "    df = df[pd.to_numeric(df[\"Value\"], errors=\"coerce\").notna()].copy()\n",
    "    df[\"Value\"] = df[\"Value\"].astype(float)\n",
    "\n",
    "    # Apply StandardScaler to the \"Value\" column\n",
    "    scaler = StandardScaler()\n",
    "    df[\"Value\"] = scaler.fit_transform(df[[\"Value\"]])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting max_sequence_length (= 1500, needed for padding the sequences). Note that set-a contains the largest sequence\n",
    "file_paths = glob.glob(\"data/set-a/*.txt\")\n",
    "max_sequence_length = 0\n",
    "\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "    df = df[df['Parameter'] != 'RecordID']\n",
    "    if(len(df) > max_sequence_length):\n",
    "        max_sequence_length = len(df)\n",
    "\n",
    "# Experimental: Clip max_sequence_length to 1000\n",
    "max_sequence_length = 1000\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all sequences and appending them to a list for extracting the embeddings\n",
    "\n",
    "time_sequences_a = []\n",
    "category_sequences_a = []\n",
    "value_sequences_a = []\n",
    "\n",
    "time_sequences_b = []\n",
    "category_sequences_b = []\n",
    "value_sequences_b = []\n",
    "\n",
    "time_sequences_c = []\n",
    "category_sequences_c = []\n",
    "value_sequences_c = []\n",
    "\n",
    "\n",
    "# Set-a processing\n",
    "file_paths = glob.glob(\"data/set-a/*.txt\")\n",
    "i = 0\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out \"ICUType\" & \"RecordID\"\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "    df = df[df['Parameter'] != 'RecordID']\n",
    "\n",
    "    # Time scaling\n",
    "    df[\"Time\"] = df[\"Time\"].apply(scale_time).astype(float)\n",
    "\n",
    "    # Parameter one-hot encoding\n",
    "    parameter_column = df[\"Parameter\"].values.reshape(-1, 1)\n",
    "    encoded_params = encoder.fit_transform(parameter_column)\n",
    "    indices = np.argmax(encoded_params, axis=1)\n",
    "    df[\"Parameter\"] = indices\n",
    "\n",
    "    # Scaling the values\n",
    "    df = scale_values(df)\n",
    "    \n",
    "    time_sequences_a.append(torch.tensor(df[\"Time\"].to_numpy()))\n",
    "    category_sequences_a.append(torch.tensor(df[\"Parameter\"].to_numpy()))\n",
    "    value_sequences_a.append(torch.tensor(df[\"Value\"].to_numpy()))\n",
    "\n",
    "print(\"Processing of Set-a done\")    \n",
    "\n",
    "\n",
    "# Set-b processing\n",
    "file_paths = glob.glob(\"data/set-b/*.txt\")\n",
    "i = 0\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out \"ICUType\" & \"RecordID\"\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "    df = df[df['Parameter'] != 'RecordID']\n",
    "\n",
    "    # Time scaling\n",
    "    df[\"Time\"] = df[\"Time\"].apply(scale_time).astype(float)\n",
    "\n",
    "    # Parameter one-hot encoding\n",
    "    parameter_column = df[\"Parameter\"].values.reshape(-1, 1)\n",
    "    encoded_params = encoder.fit_transform(parameter_column)\n",
    "    indices = np.argmax(encoded_params, axis=1)\n",
    "    df[\"Parameter\"] = indices\n",
    "\n",
    "    # Scaling the values\n",
    "    df = scale_values(df)\n",
    "    \n",
    "    time_sequences_b.append(torch.tensor(df[\"Time\"].to_numpy()))\n",
    "    category_sequences_b.append(torch.tensor(df[\"Parameter\"].to_numpy()))\n",
    "    value_sequences_b.append(torch.tensor(df[\"Value\"].to_numpy()))\n",
    "\n",
    "print(\"Processing of Set-b done\")\n",
    "\n",
    "\n",
    "# Set-c processing\n",
    "file_paths = glob.glob(\"data/set-c/*.txt\")\n",
    "i = 0\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out \"ICUType\" & \"RecordID\"\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "    df = df[df['Parameter'] != 'RecordID']\n",
    "    df = df[df['Parameter'] != '']\n",
    "\n",
    "    # Remove nans in parameter column\n",
    "    df = df.dropna(subset=[\"Parameter\"])\n",
    "\n",
    "    # Time scaling\n",
    "    df[\"Time\"] = df[\"Time\"].apply(scale_time).astype(float)\n",
    "\n",
    "    # Parameter one-hot encoding\n",
    "    parameter_column = df[\"Parameter\"].values.reshape(-1, 1)\n",
    "    encoded_params = encoder.fit_transform(parameter_column)\n",
    "    indices = np.argmax(encoded_params, axis=1)\n",
    "    df[\"Parameter\"] = indices\n",
    "\n",
    "    # Scaling the values\n",
    "    df = scale_values(df)\n",
    "    \n",
    "    time_sequences_c.append(torch.tensor(df[\"Time\"].to_numpy()))\n",
    "    category_sequences_c.append(torch.tensor(df[\"Parameter\"].to_numpy()))\n",
    "    value_sequences_c.append(torch.tensor(df[\"Value\"].to_numpy()))\n",
    "\n",
    "print(\"Processing of Set-c done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variables = 40\n",
    "embedding_dim = 20\n",
    "\n",
    "# Embedding layer for categorical/parameter variables\n",
    "variable_embedding = nn.Embedding(num_variables, embedding_dim)\n",
    "\n",
    "# Linear layer to project (time, value) into the same space\n",
    "time_value_proj = nn.Linear(2, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(time_sequences, category_sequences, value_sequences):\n",
    "    # Convert categorical variables to embeddings\n",
    "    var_embeddings = [variable_embedding(seq) for seq in category_sequences]\n",
    "\n",
    "    # Stack and project time and value\n",
    "    time_value_seqs = [torch.cat([t.unsqueeze(1), v.unsqueeze(1)], dim=1).to(torch.float32) for t, v in zip(time_sequences, value_sequences)]\n",
    "    time_value_embeddings = [time_value_proj(seq) for seq in time_value_seqs]\n",
    "\n",
    "    # Concatenate everything\n",
    "    final_sequences = [torch.cat([var_emb, tv_emb], dim=1) \n",
    "                       for var_emb, tv_emb in zip(var_embeddings, time_value_embeddings)]\n",
    "\n",
    "    # Pad sequences to a fixed length of max_sequence_length\n",
    "    max_len = max_sequence_length\n",
    "    padded_sequences = torch.zeros(len(final_sequences), max_len, final_sequences[0].size(1))\n",
    "\n",
    "    for i, seq in enumerate(final_sequences):\n",
    "        length = min(seq.size(0), max_len)\n",
    "        padded_sequences[i, :length, :] = seq[:length]\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting embeddings which are the inputs of the transformer\n",
    "padded_sequences_a = process_patient_data(time_sequences_a, category_sequences_a, value_sequences_a)\n",
    "padded_sequences_b = process_patient_data(time_sequences_b, category_sequences_b, value_sequences_b)\n",
    "padded_sequences_c = process_patient_data(time_sequences_c, category_sequences_c, value_sequences_c)\n",
    "print(padded_sequences_a.shape)\n",
    "print(padded_sequences_b.shape)\n",
    "print(padded_sequences_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientSequences(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings.clone().detach().float()\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "def get_dataloader(embeddings, labels, batch_size=32, shuffle=True):\n",
    "    dataset = PatientSequences(embeddings, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sequences_a\n",
    "y = pd.read_parquet('processed-data/processed-outcomes-a.parquet')\n",
    "y = y[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "train_dataset = PatientSequences(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "X = padded_sequences_b\n",
    "y = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y = y[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "val_dataset = PatientSequences(X, y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "X = padded_sequences_c\n",
    "y = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y = y[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "test_dataset = PatientSequences(X, y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=max_sequence_length):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.shape[1], :]\n",
    "\n",
    "class PreNormTransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x), attn_mask=mask, need_weights=False)[0]\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim=50, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.2, max_sequence_length=max_sequence_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(input_dim, max_sequence_length)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            PreNormTransformerLayer(input_dim, nhead, dim_feedforward, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 40\n",
    "hidden_dim = 256         \n",
    "num_heads = 4          \n",
    "num_layers = 4        \n",
    "learning_rate = 0.0005\n",
    "num_epochs = 10\n",
    "\n",
    "# Test AUROC: 0.7714, Test AUPRC: 0.3392\n",
    "\"\"\" input_dim = 40\n",
    "hidden_dim = 256         \n",
    "num_heads = 4          \n",
    "num_layers = 4        \n",
    "learning_rate = 0.0005\n",
    "num_epochs = 11 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(input_dim, num_heads, num_layers).to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and validation loss\n",
    "\n",
    "# NOTE: This is for validation error analysis\n",
    "\n",
    "total_val_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    total_val_loss += val_loss\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "total_val_loss /= num_epochs\n",
    "print(f\"Avg val loss: {total_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs).squeeze()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Compute AUROC & AUPRC\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
