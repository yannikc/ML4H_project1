{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import glob\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Classic Machine Learning Methods (5 Pts)\n",
    "#### Q2.1 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the X matrix (can use)\n",
    "df = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "df = df.fillna(0)\n",
    "X = df.groupby(\"RecordID\").last(numeric_only=True).reset_index()\n",
    "X = X.drop(columns=[\"RecordID\"])\n",
    "X = X[sorted(X.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the label vector\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-a.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "print(y.sum())\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small note: We have observed that there is a class imbalance in the dataset. Out of 4000 entries, only 554 contain a 1, whereas the rest consists of 0's. In Q2.1 1. We compare results from taking class imbalance into account (using SMOTE) and simply igroning it (without SMOTE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking class imbalance into account (there are far more 1's than 0's in the \"In-hospital_death\" column)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE on the entire dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models (WITH taking class imbalance into account)\n",
    "# Logistic Regression\n",
    "model1 = LogisticRegression(max_iter=500)\n",
    "model1.fit(X_resampled,y_resampled)\n",
    "\n",
    "# Random Forest\n",
    "model2 = RandomForestClassifier()\n",
    "model2.fit(X_resampled,y_resampled)\n",
    "\n",
    "# KNN\n",
    "model3 = KNeighborsClassifier()\n",
    "model3.fit(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models (WITHOUT taking class imbalance into account)\n",
    "# Logistic Regression\n",
    "model4 = LogisticRegression(max_iter=4300)\n",
    "model4.fit(X,y)\n",
    "\n",
    "# Random Forest\n",
    "model5 = RandomForestClassifier()\n",
    "model5.fit(X,y)\n",
    "\n",
    "# KNN\n",
    "model6 = KNeighborsClassifier()\n",
    "model6.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set C performance (WITH SMOTE)\n",
    "\n",
    "# Loading test set C\n",
    "df = pd.read_parquet('final-data/final-set-c.parquet')\n",
    "df = df.fillna(0)\n",
    "df = df.drop(columns=[\"ICUType\"])\n",
    "X_test = df.groupby(\"RecordID\").last(numeric_only=True).reset_index()\n",
    "X_test = X_test.drop(columns=[\"RecordID\"])\n",
    "X_test= X_test[sorted(X_test.columns)]\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y_test = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "y_pred1 = model1.predict_proba(X_test)[:,1]\n",
    "y_pred2 = model2.predict_proba(X_test)[:,1]\n",
    "y_pred3 = model3.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Logistic Regression\n",
    "print(\"Logistic Regression results WITH SMOTE\")\n",
    "auroc = roc_auc_score(y_test, y_pred1)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred1)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Random Forests\n",
    "print(\"Random Forests results WITH SMOTE\")\n",
    "auroc = roc_auc_score(y_test, y_pred2)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred2)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for KNN\n",
    "print(\"KNN results WITH SMOTE\")\n",
    "auroc = roc_auc_score(y_test, y_pred3)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred3)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set C performance (WITHOUT SMOTE)\n",
    "\n",
    "# Loading test set C\n",
    "df = pd.read_parquet('final-data/final-set-c.parquet')\n",
    "df = df.fillna(0)\n",
    "df = df.drop(columns=[\"ICUType\"])\n",
    "X_test = df.groupby(\"RecordID\").last(numeric_only=True).reset_index()\n",
    "X_test = X_test.drop(columns=[\"RecordID\"])\n",
    "X_test= X_test[sorted(X_test.columns)]\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y_test = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "y_pred1 = model4.predict_proba(X_test)[:,1]\n",
    "y_pred2 = model5.predict_proba(X_test)[:,1]\n",
    "y_pred3 = model6.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Logistic Regression\n",
    "print(\"Logistic Regression results WITHOUT SMOTE\")\n",
    "auroc = roc_auc_score(y_test, y_pred1)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred1)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Random Forests\n",
    "print(\"Random Forests results WITHOUT SMOTE\")\n",
    "auroc = roc_auc_score(y_test, y_pred2)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred2)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for KNN\n",
    "print(\"KNN results WITHOUT SMOTE\")\n",
    "auroc = roc_auc_score(y_test, y_pred3)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred3)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.1 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_settings = MinimalFCParameters()\n",
    "\n",
    "# Extracting features of concatenated training and test dataset (need to do this in one go so the feature extraction is consistent)\n",
    "df_train = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "df_test = pd.read_parquet('final-data/final-set-c.parquet').drop(columns=[\"ICUType\"])\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)\n",
    "\n",
    "X_train = extract_features(df_train, column_id='RecordID', column_sort='Time', default_fc_parameters=extraction_settings, impute_function=impute)\n",
    "X_test = extract_features(df_test, column_id='RecordID', column_sort='Time', default_fc_parameters=extraction_settings, impute_function=impute)\n",
    "\n",
    "X_train= X_train[sorted(X_train.columns)]\n",
    "X_test= X_test[sorted(X_test.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet('processed-data/processed-outcomes-a.parquet')[\"In-hospital_death\"].to_numpy().flatten()\n",
    "y_test = pd.read_parquet('processed-data/processed-outcomes-c.parquet')[\"In-hospital_death\"].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "# Logistic Regression\n",
    "model1 = LogisticRegression(max_iter=10000)\n",
    "model1.fit(X_train,y_train)\n",
    "\n",
    "# Random Forest\n",
    "model2 = RandomForestClassifier()\n",
    "model2.fit(X_train,y_train)\n",
    "\n",
    "# KNN\n",
    "model3 = KNeighborsClassifier()\n",
    "model3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict_proba(X_test)[:,1]\n",
    "y_pred2 = model2.predict_proba(X_test)[:,1]\n",
    "y_pred3 = model3.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Logistic Regression\n",
    "print(\"Logistic Regression results\")\n",
    "auroc = roc_auc_score(y_test, y_pred1)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred1)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for Random Forests\n",
    "print(\"Random Forests results\")\n",
    "auroc = roc_auc_score(y_test, y_pred2)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred2)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")\n",
    "\n",
    "# Calculation of AuROC and AuPRC for KNN\n",
    "print(\"KNN results\")\n",
    "auroc = roc_auc_score(y_test, y_pred3)\n",
    "print(f\"AUROC: {auroc}\")\n",
    "auprc = average_precision_score(y_test, y_pred3)\n",
    "print(f\"AUPRC: {auprc}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Recurrent Neural Networks (4 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in case CUDA_LAUNCH_BLOCKING error occurs\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.7)\n",
    "        self.bn = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Extracting output of last timestep\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "\n",
    "        out = self.fc(lstm_out)  \n",
    "        return out\n",
    "    \n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.7, bidirectional=True)\n",
    "\n",
    "        # Adjusted because of bidirectional LSTM\n",
    "        self.bn = nn.LayerNorm(hidden_size * 2)  \n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Extracting output of last timestep\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "##### NORMAL LSTM\n",
    "# AUROC 0.8204 (Only set a)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "num_epochs = 10 \"\"\"\n",
    "\n",
    "# AUROC 0.7483 (set a and set b)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0005\n",
    "batch_size = 64\n",
    "num_epochs = 10 \"\"\"\n",
    "\n",
    "##### BIDIRECTIONAL LSTM\n",
    "# AUROC 0.8226 (Only set a)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.0003\n",
    "batch_size = 64\n",
    "num_epochs = 3 \"\"\"\n",
    "\n",
    "# AUROC 0.7773 (set a and set b)\n",
    "\"\"\" input_size = 41\n",
    "hidden_size = 256\n",
    "num_layers = 8\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 2 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: In the scaled-data set the time column is not scaled. However, due to better performance, we still scale it for this application. RecordID can be dropped, since it doesn't convey any further information and each patient is assigned one of the 4000 dimensions.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Training Data\n",
    "df = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "X = df.fillna(0)\n",
    "X = X.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X = X.sort_values(by=\"RecordID\", ascending=True)\n",
    "X[\"Time\"] = scaler.fit_transform(X[[\"Time\"]])\n",
    "X = X.drop(columns=[\"RecordID\"])\n",
    "X = X[sorted(X.columns)]\n",
    "X = X.to_numpy()\n",
    "X = X.reshape(4000,49,41)\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-a.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "train_dataset = PatientDataset(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Validation data\n",
    "df = pd.read_parquet('final-data/final-set-b.parquet')\n",
    "X = df.fillna(0)\n",
    "X = X.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X = X.sort_values(by=\"RecordID\", ascending=True)\n",
    "X[\"Time\"] = scaler.fit_transform(X[[\"Time\"]])\n",
    "X = X.drop(columns=[\"RecordID\"]).drop(columns=[\"ICUType\"])\n",
    "X = X[sorted(X.columns)]\n",
    "X = X.to_numpy()\n",
    "X = X.reshape(4000,49,41)\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "val_dataset = PatientDataset(X, y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Test data\n",
    "df = pd.read_parquet('final-data/final-set-c.parquet')\n",
    "X = df.fillna(0)\n",
    "X = X.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X = X.sort_values(by=\"RecordID\", ascending=True)\n",
    "X[\"Time\"] = scaler.fit_transform(X[[\"Time\"]])\n",
    "X = X.drop(columns=[\"RecordID\"]).drop(columns=[\"ICUType\"])\n",
    "X = X[sorted(X.columns)]\n",
    "X = X.to_numpy()\n",
    "X = X.reshape(4000,49,41)\n",
    "\n",
    "y_df = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y = y_df[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "test_dataset = PatientDataset(X, y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Full training (use after having found best model with lowest valildation error)\n",
    "df = pd.read_parquet('final-data/final-set-a.parquet')\n",
    "X1 = df.fillna(0)\n",
    "X1 = X1.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X1 = X1.sort_values(by=\"RecordID\", ascending=True)\n",
    "X1[\"Time\"] = scaler.fit_transform(X1[[\"Time\"]])\n",
    "X1 = X1[sorted(X1.columns)]\n",
    "df = pd.read_parquet('final-data/final-set-b.parquet')\n",
    "X2 = df.fillna(0)\n",
    "X2 = X2.groupby(\"RecordID\").tail(49).reset_index(drop=True)\n",
    "X2 = X2.sort_values(by=\"RecordID\", ascending=True)\n",
    "X2[\"Time\"] = scaler.fit_transform(X2[[\"Time\"]])\n",
    "X2 = X2.drop(columns=[\"ICUType\"])\n",
    "X2 = X2[sorted(X2.columns)]\n",
    "\n",
    "X_full = pd.concat([X1, X2], axis=0).drop(columns=\"RecordID\")\n",
    "\n",
    "X_full = X_full.to_numpy()\n",
    "X_full = X_full.reshape(8000,49,41)\n",
    "\n",
    "y1 = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y1 = y1[\"In-hospital_death\"].to_numpy().flatten()\n",
    "y2 = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y2 = y2[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "y_full = np.concatenate((y1,y2), axis=0)\n",
    "\n",
    "full_dataset = PatientDataset(X_full, y_full)\n",
    "full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Uncomment/comment line depending on if you want to use normal LSTM or bidirectional LSTM\n",
    "#model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "model = BidirectionalLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and validation loss\n",
    "\n",
    "# NOTE: This is for validation error analysis\n",
    "\n",
    "total_val_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(test_loader)\n",
    "    total_val_loss += val_loss\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "total_val_loss /= num_epochs\n",
    "print(f\"Avg val loss: {total_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and AUROC/AUPRC\n",
    "\n",
    "# NOTE: Change train_loader with full_loader if we want to train the model on set a and set b (after validation error analysis)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # AUROC & AUPRC calculation\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            probs = torch.sigmoid(outputs).squeeze()  # Convert logits to probabilities (since we use BCEWithLogitsLoss() loss function)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3a Transformers (3 Pts) (MUCH FASTER TO TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, d_model=128, num_heads=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, # Dimensionality of embeddings\n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # FC layer for classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_heads=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Set d_model equal to input_size to avoid embedding projection\n",
    "        d_model = input_size  \n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # FC layer for classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)  # Process directly without embedding\n",
    "        x = x.mean(dim=1)  # Global pooling over time steps\n",
    "        x = self.fc(x).squeeze()  # Final classification layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = 41\n",
    "num_classes = 1 \n",
    "d_model = 128\n",
    "num_heads = 41\n",
    "num_layers = 8\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0004\n",
    "num_epochs = 25\n",
    "\n",
    "# AUROC 0.8396 (Only set a)\n",
    "\"\"\" input_size = 41\n",
    "num_classes = 1 \n",
    "d_model = 128\n",
    "num_heads = 16\n",
    "num_layers = 8\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 6 \"\"\"\n",
    "\n",
    "# AUROC 0.8383 (set a and set b)\n",
    "\"\"\" input_size = 41\n",
    "num_classes = 1 \n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 4 \n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 4 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = Transformer(input_size, num_classes, d_model, num_heads, num_layers, dim_feedforward, dropout).to(device)\n",
    "model = Transformer(input_size, num_classes, num_heads, num_layers, dim_feedforward, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([7.0]).to(device))  # Handle class imbalance\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop which prints current training loss and AUROC/AUPRC\n",
    "\n",
    "# NOTE: Change train_loader with full_loader if we want to train the model on set a and set b (after validation error analysis)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_probs = []  # Store probabilities for AUROC & AUPRC calculation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            probs = torch.sigmoid(outputs).squeeze()  # Convert logits to probabilities\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "            all_probs.extend(probs.cpu().numpy())  # Store predicted probabilities\n",
    "\n",
    "    # Compute AUROC & AUPRC\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3b Tokenizing Time-Series Data and Transformers (4 Pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time conversion from \"XX:YY\" to an interval between [0,1]\n",
    "def scale_time(time_str):\n",
    "    # Split the time string into hours and minutes\n",
    "    hours, minutes = map(int, time_str.split(\":\"))\n",
    "    \n",
    "    # Convert the time to total minutes\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    \n",
    "    # Maximum time is 48 hours (2880 minutes)\n",
    "    max_time = 48 * 60\n",
    "    \n",
    "    # Scale time to the range [0, 1]\n",
    "    scaled_time = total_minutes / max_time\n",
    "    return scaled_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding of the different categories\n",
    "categories = [['ALP', 'ALT', 'AST', 'Age', 'Albumin', 'BUN', 'Bilirubin', 'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2',\n",
    "                'GCS', 'Gender', 'Glucose', 'HCO3', 'HCT', 'HR', 'Height', 'K', 'Lactate', 'MAP', 'MechVent', 'Mg', 'NIDiasABP',\n",
    "                'NIMAP', 'NISysABP', 'Na', 'PaCO2', 'PaO2', 'Platelets', 'RecordID', 'RespRate', 'SaO2', 'SysABP', 'Temp', 'TroponinI',\n",
    "                'TroponinT', 'Urine', 'WBC', 'Weight', 'pH']]\n",
    "encoder = OneHotEncoder(categories=categories, sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_values(df):\n",
    "    # Ensure the \"Value\" column is numeric\n",
    "    df = df[pd.to_numeric(df[\"Value\"], errors=\"coerce\").notna()].copy()\n",
    "    df[\"Value\"] = df[\"Value\"].astype(float)\n",
    "\n",
    "    # Apply StandardScaler to the \"Value\" column\n",
    "    scaler = StandardScaler()\n",
    "    df[\"Value\"] = scaler.fit_transform(df[[\"Value\"]])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501\n"
     ]
    }
   ],
   "source": [
    "# Getting max_sequence_length (= 1501, needed for padding the sequences). Note that set-a contains the largest sequence\n",
    "file_paths = glob.glob(\"data/set-a/*.txt\")\n",
    "max_sequence_length = 0\n",
    "\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "    if(len(df) > max_sequence_length):\n",
    "        max_sequence_length = len(df)\n",
    "\n",
    "print(max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of Set-a done\n",
      "Processing of Set-b done\n",
      "Processing of Set-c done\n"
     ]
    }
   ],
   "source": [
    "# Processing all sequences and appending them to a list for extracting the embeddings\n",
    "\n",
    "time_sequences_a = []\n",
    "category_sequences_a = []\n",
    "value_sequences_a = []\n",
    "\n",
    "time_sequences_b = []\n",
    "category_sequences_b = []\n",
    "value_sequences_b = []\n",
    "\n",
    "time_sequences_c = []\n",
    "category_sequences_c = []\n",
    "value_sequences_c = []\n",
    "\n",
    "\n",
    "# Set-a processing\n",
    "file_paths = glob.glob(\"data/set-a/*.txt\")\n",
    "i = 0\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out \"ICUType\"\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "\n",
    "    # Time scaling\n",
    "    df[\"Time\"] = df[\"Time\"].apply(scale_time).astype(float)\n",
    "\n",
    "    # Parameter one-hot encoding\n",
    "    parameter_column = df[\"Parameter\"].values.reshape(-1, 1)\n",
    "    encoded_params = encoder.fit_transform(parameter_column)\n",
    "    indices = np.argmax(encoded_params, axis=1)\n",
    "    df[\"Parameter\"] = indices\n",
    "\n",
    "    # Scaling the values\n",
    "    df = scale_values(df)\n",
    "    \n",
    "    time_sequences_a.append(torch.tensor(df[\"Time\"].to_numpy()))\n",
    "    category_sequences_a.append(torch.tensor(df[\"Parameter\"].to_numpy()))\n",
    "    value_sequences_a.append(torch.tensor(df[\"Value\"].to_numpy()))\n",
    "\n",
    "print(\"Processing of Set-a done\")    \n",
    "\n",
    "\n",
    "# Set-b processing\n",
    "file_paths = glob.glob(\"data/set-b/*.txt\")\n",
    "i = 0\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out \"ICUType\"\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "\n",
    "    # Time scaling\n",
    "    df[\"Time\"] = df[\"Time\"].apply(scale_time).astype(float)\n",
    "\n",
    "    # Parameter one-hot encoding\n",
    "    parameter_column = df[\"Parameter\"].values.reshape(-1, 1)\n",
    "    encoded_params = encoder.fit_transform(parameter_column)\n",
    "    indices = np.argmax(encoded_params, axis=1)\n",
    "    df[\"Parameter\"] = indices\n",
    "\n",
    "    # Scaling the values\n",
    "    df = scale_values(df)\n",
    "    \n",
    "    time_sequences_b.append(torch.tensor(df[\"Time\"].to_numpy()))\n",
    "    category_sequences_b.append(torch.tensor(df[\"Parameter\"].to_numpy()))\n",
    "    value_sequences_b.append(torch.tensor(df[\"Value\"].to_numpy()))\n",
    "\n",
    "print(\"Processing of Set-b done\")\n",
    "\n",
    "\n",
    "# Set-c processing\n",
    "file_paths = glob.glob(\"data/set-c/*.txt\")\n",
    "i = 0\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out \"ICUType\"\n",
    "    df = df[df['Parameter'] != 'ICUType']\n",
    "    df = df[df['Parameter'] != '']\n",
    "\n",
    "    # Remove nans in parameter column\n",
    "    df = df.dropna(subset=[\"Parameter\"])\n",
    "\n",
    "    # Time scaling\n",
    "    df[\"Time\"] = df[\"Time\"].apply(scale_time).astype(float)\n",
    "\n",
    "    # Parameter one-hot encoding\n",
    "    parameter_column = df[\"Parameter\"].values.reshape(-1, 1)\n",
    "    encoded_params = encoder.fit_transform(parameter_column)\n",
    "    indices = np.argmax(encoded_params, axis=1)\n",
    "    df[\"Parameter\"] = indices\n",
    "\n",
    "    # Scaling the values\n",
    "    df = scale_values(df)\n",
    "    \n",
    "    time_sequences_c.append(torch.tensor(df[\"Time\"].to_numpy()))\n",
    "    category_sequences_c.append(torch.tensor(df[\"Parameter\"].to_numpy()))\n",
    "    value_sequences_c.append(torch.tensor(df[\"Value\"].to_numpy()))\n",
    "\n",
    "print(\"Processing of Set-c done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variables = 41\n",
    "embedding_dim = 20\n",
    "\n",
    "# Embedding layer for categorical/parameter variables\n",
    "variable_embedding = nn.Embedding(num_variables, embedding_dim)\n",
    "\n",
    "# Linear layer to project (time, value) into the same space\n",
    "time_value_proj = nn.Linear(2, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(time_sequences, category_sequences, value_sequences):\n",
    "    # Convert categorical variables to embeddings\n",
    "    var_embeddings = [variable_embedding(seq) for seq in category_sequences]\n",
    "\n",
    "    # Stack and project time and value\n",
    "    time_value_seqs = [torch.cat([t.unsqueeze(1), v.unsqueeze(1)], dim=1).to(torch.float32) for t, v in zip(time_sequences, value_sequences)]\n",
    "    time_value_embeddings = [time_value_proj(seq) for seq in time_value_seqs]\n",
    "\n",
    "    # Concatenate everything\n",
    "    final_sequences = [torch.cat([var_emb, tv_emb], dim=1) \n",
    "                       for var_emb, tv_emb in zip(var_embeddings, time_value_embeddings)]\n",
    "\n",
    "    # Pad sequences to a fixed length of 1501\n",
    "    max_len = max_sequence_length\n",
    "    padded_sequences = torch.zeros(len(final_sequences), max_len, final_sequences[0].size(1))\n",
    "\n",
    "    for i, seq in enumerate(final_sequences):\n",
    "        length = min(seq.size(0), max_len)\n",
    "        padded_sequences[i, :length, :] = seq[:length]\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 1501, 40])\n",
      "torch.Size([4000, 1501, 40])\n",
      "torch.Size([4000, 1501, 40])\n"
     ]
    }
   ],
   "source": [
    "# Getting embeddings which are the inputs of the transformer\n",
    "padded_sequences_a = process_patient_data(time_sequences_a, category_sequences_a, value_sequences_a)\n",
    "padded_sequences_b = process_patient_data(time_sequences_b, category_sequences_b, value_sequences_b)\n",
    "padded_sequences_c = process_patient_data(time_sequences_c, category_sequences_c, value_sequences_c)\n",
    "print(padded_sequences_a.shape)\n",
    "print(padded_sequences_b.shape)\n",
    "print(padded_sequences_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientSequences(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings.clone().detach().float()\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "def get_dataloader(embeddings, labels, batch_size=32, shuffle=True):\n",
    "    dataset = PatientSequences(embeddings, labels)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sequences_a\n",
    "y = pd.read_parquet('processed-data/processed-outcomes-a.parquet')\n",
    "y = y[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "train_dataset = PatientSequences(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "X = padded_sequences_b\n",
    "y = pd.read_parquet('processed-data/processed-outcomes-b.parquet')\n",
    "y = y[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "val_dataset = PatientSequences(X, y)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "X = padded_sequences_c\n",
    "y = pd.read_parquet('processed-data/processed-outcomes-c.parquet')\n",
    "y = y[\"In-hospital_death\"].to_numpy().flatten()\n",
    "\n",
    "test_dataset = PatientSequences(X, y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim=40, d_model=40, nhead=4, num_layers=3, dim_feedforward=128, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 1501, input_dim))  # Learnable Positional Encoding\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 40\n",
    "d_model = 128\n",
    "hidden_dim = 64         \n",
    "num_heads = 4           \n",
    "num_layers = 4          \n",
    "learning_rate = 0.001\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(input_dim, d_model, num_heads, num_layers).to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4089, Test AUROC: 0.5875, Test AUPRC: 0.2077\n",
      "Epoch [2/20], Loss: 0.4033, Test AUROC: 0.5953, Test AUPRC: 0.2132\n",
      "Epoch [3/20], Loss: 0.4042, Test AUROC: 0.5965, Test AUPRC: 0.2158\n",
      "Epoch [4/20], Loss: 0.4021, Test AUROC: 0.6033, Test AUPRC: 0.2195\n",
      "Epoch [5/20], Loss: 0.3993, Test AUROC: 0.6187, Test AUPRC: 0.2270\n",
      "Epoch [6/20], Loss: 0.4008, Test AUROC: 0.6454, Test AUPRC: 0.2391\n",
      "Epoch [7/20], Loss: 0.3941, Test AUROC: 0.6536, Test AUPRC: 0.2330\n",
      "Epoch [8/20], Loss: 0.3869, Test AUROC: 0.6523, Test AUPRC: 0.2204\n",
      "Epoch [9/20], Loss: 0.3833, Test AUROC: 0.6592, Test AUPRC: 0.2246\n",
      "Epoch [10/20], Loss: 0.3779, Test AUROC: 0.6712, Test AUPRC: 0.2380\n",
      "Epoch [11/20], Loss: 0.3702, Test AUROC: 0.6654, Test AUPRC: 0.2333\n",
      "Epoch [12/20], Loss: 0.3656, Test AUROC: 0.6643, Test AUPRC: 0.2376\n",
      "Epoch [13/20], Loss: 0.3597, Test AUROC: 0.6597, Test AUPRC: 0.2344\n",
      "Epoch [14/20], Loss: 0.3580, Test AUROC: 0.6583, Test AUPRC: 0.2354\n",
      "Epoch [15/20], Loss: 0.3463, Test AUROC: 0.6461, Test AUPRC: 0.2339\n",
      "Epoch [16/20], Loss: 0.3396, Test AUROC: 0.6506, Test AUPRC: 0.2329\n",
      "Epoch [17/20], Loss: 0.3401, Test AUROC: 0.6612, Test AUPRC: 0.2404\n",
      "Epoch [18/20], Loss: 0.3284, Test AUROC: 0.6493, Test AUPRC: 0.2359\n",
      "Epoch [19/20], Loss: 0.3380, Test AUROC: 0.6418, Test AUPRC: 0.2347\n",
      "Epoch [20/20], Loss: 0.3192, Test AUROC: 0.6274, Test AUPRC: 0.2208\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_probs = []  # Store probabilities for AUROC & AUPRC calculation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            probs = torch.sigmoid(outputs).squeeze()  # Convert logits to probabilities\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())  # Store true labels\n",
    "            all_probs.extend(probs.cpu().numpy())  # Store predicted probabilities\n",
    "\n",
    "    # Compute AUROC & AUPRC\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    auprc = average_precision_score(all_labels, all_probs)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
